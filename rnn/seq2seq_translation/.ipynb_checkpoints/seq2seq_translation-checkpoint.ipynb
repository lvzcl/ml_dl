{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英法翻译的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "with open('./data/small_vocab_en', 'r') as f:\n",
    "    english_data = f.read()\n",
    "f.close()\n",
    "with open('./data/small_vocab_fr', 'r') as f:\n",
    "    french_data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the num of english sentences is 137861\n",
      "the num of french sentences is 137861\n",
      "the max word number of english sentences is 17\n",
      "the max word number of french sentences is 23\n"
     ]
    }
   ],
   "source": [
    "#简单分析下\n",
    "english_sentences = english_data.split('\\n')\n",
    "french_sentences = french_data.split('\\n')\n",
    "print('the num of english sentences is {}'.format(len(english_sentences)))\n",
    "print('the num of french sentences is {}'.format(len(french_sentences)))\n",
    "english_word_max_number = max([len(sentence.split()) for sentence in english_sentences])\n",
    "french_word_max_number = max([len(sentence.split()) for sentence in french_sentences])\n",
    "print('the max word number of english sentences is {}'.format(english_word_max_number))\n",
    "print('the max word number of french sentences is {}'.format(french_word_max_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊字符\n",
    "    <PAD>：由于翻译问题的特殊性，我们的句子长度往往是不一致的，而在RNN处理batch数据时，我们需要保证batch中的句子长度一致，此时需要通过<PAD>对长度不足的句子进行补全；\n",
    "    <UNK>：Unknown字符，用来处理模型未见过的生僻单词；\n",
    "    <GO>：翻译句子时，用来告诉句子开始进行翻译。仅在target中使用；\n",
    "    <EOS>：翻译句子时，用来告诉句子结束翻译。仅在target中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of english vocab is 227\n",
      "the size of french vocab is 354\n"
     ]
    }
   ],
   "source": [
    "#构建语料库，将English_data 和 french_data 转换成数字格式\n",
    "english_vocab = list(set(english_data.lower().split()))\n",
    "french_vocab = list(set(french_data.lower().split()))\n",
    "print('the size of english vocab is {}'.format(len(english_vocab)))\n",
    "print('the size of french vocab is {}'.format(len(french_vocab)))\n",
    "source_special_words = ['<PAD>', '<UNK>']\n",
    "target_special_words = ['<PAD>', '<UNK>', '<GO>', '<EOS>']\n",
    "#英语是source， 法语为target\n",
    "source_int_to_word = {idx:word for idx, word in enumerate(english_vocab + source_special_words)}\n",
    "source_word_to_int = {word:idx for idx, word in source_int_to_word.items()}\n",
    "\n",
    "target_int_to_word = {idx:word for idx, word in enumerate(french_vocab + target_special_words)}\n",
    "target_word_to_int = {word:idx for idx, word in target_int_to_word.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下来进行数据格式的转换，将其转换成数字格式\n",
    "def text_to_int(sentence, map_dict, max_length=20, is_target=True):\n",
    "    '''\n",
    "    sentence:输入的是一个句子，\n",
    "    map_dict:是存储word 和 int 转换的字典\n",
    "    max_length：应为LSTM需要处理定长数据，给定一个最大长度，未到达这个长度的就进行补<PAD>操作\n",
    "    is_target:用来确定是source还是target\n",
    "    '''\n",
    "    sentence_with_int = []\n",
    "    if not is_target:\n",
    "        for word in sentence.lower().split():\n",
    "            sentence_with_int.append(map_dict.get(word, map_dict['<UNK>']))\n",
    "    else:\n",
    "        for word in sentence.lower().split():\n",
    "            sentence_with_int.append(map_dict.get(word, map_dict['<UNK>']))\n",
    "        sentence_with_int.append(map_dict['<EOS>'])\n",
    "    if len(sentence_with_int) > max_length:\n",
    "        return sentence_with_int[:max_length]\n",
    "    else:\n",
    "        return sentence_with_int + [map_dict['<PAD>']] * (max_length - len(sentence_with_int))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text_to_int = []\n",
    "for sentence in english_data.split('\\n'):\n",
    "    source_text_to_int.append(text_to_int(sentence, source_word_to_int, max_length=20, is_target=False))\n",
    "target_text_to_int = []\n",
    "for sentence in french_data.split('\\n'):\n",
    "    target_text_to_int.append(text_to_int(sentence, target_word_to_int, max_length=25, is_target=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137861"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "137861"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[161,\n",
       "  189,\n",
       "  218,\n",
       "  59,\n",
       "  83,\n",
       "  220,\n",
       "  20,\n",
       "  166,\n",
       "  156,\n",
       "  73,\n",
       "  218,\n",
       "  202,\n",
       "  163,\n",
       "  210,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [13,\n",
       "  126,\n",
       "  11,\n",
       "  218,\n",
       "  207,\n",
       "  19,\n",
       "  220,\n",
       "  103,\n",
       "  166,\n",
       "  156,\n",
       "  73,\n",
       "  218,\n",
       "  207,\n",
       "  53,\n",
       "  163,\n",
       "  92,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [148,\n",
       "  218,\n",
       "  207,\n",
       "  83,\n",
       "  220,\n",
       "  221,\n",
       "  166,\n",
       "  156,\n",
       "  73,\n",
       "  218,\n",
       "  207,\n",
       "  62,\n",
       "  163,\n",
       "  58,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [13,\n",
       "  126,\n",
       "  11,\n",
       "  218,\n",
       "  59,\n",
       "  206,\n",
       "  220,\n",
       "  58,\n",
       "  166,\n",
       "  156,\n",
       "  73,\n",
       "  218,\n",
       "  175,\n",
       "  163,\n",
       "  3,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [135,\n",
       "  49,\n",
       "  51,\n",
       "  35,\n",
       "  218,\n",
       "  13,\n",
       "  90,\n",
       "  166,\n",
       "  29,\n",
       "  180,\n",
       "  49,\n",
       "  51,\n",
       "  218,\n",
       "  13,\n",
       "  208,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [52,\n",
       "  21,\n",
       "  35,\n",
       "  218,\n",
       "  13,\n",
       "  25,\n",
       "  166,\n",
       "  29,\n",
       "  180,\n",
       "  21,\n",
       "  218,\n",
       "  13,\n",
       "  90,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [142,\n",
       "  218,\n",
       "  119,\n",
       "  220,\n",
       "  32,\n",
       "  166,\n",
       "  29,\n",
       "  73,\n",
       "  218,\n",
       "  207,\n",
       "  19,\n",
       "  163,\n",
       "  103,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [161,\n",
       "  189,\n",
       "  218,\n",
       "  183,\n",
       "  220,\n",
       "  14,\n",
       "  166,\n",
       "  156,\n",
       "  73,\n",
       "  218,\n",
       "  200,\n",
       "  62,\n",
       "  163,\n",
       "  221,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [89,\n",
       "  49,\n",
       "  51,\n",
       "  35,\n",
       "  218,\n",
       "  13,\n",
       "  6,\n",
       "  166,\n",
       "  29,\n",
       "  180,\n",
       "  49,\n",
       "  51,\n",
       "  218,\n",
       "  13,\n",
       "  90,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227,\n",
       "  227],\n",
       " [13,\n",
       "  126,\n",
       "  11,\n",
       "  218,\n",
       "  59,\n",
       "  183,\n",
       "  220,\n",
       "  146,\n",
       "  166,\n",
       "  156,\n",
       "  73,\n",
       "  218,\n",
       "  59,\n",
       "  130,\n",
       "  163,\n",
       "  92,\n",
       "  160,\n",
       "  227,\n",
       "  227,\n",
       "  227]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[327,\n",
       "  248,\n",
       "  206,\n",
       "  292,\n",
       "  238,\n",
       "  16,\n",
       "  265,\n",
       "  37,\n",
       "  233,\n",
       "  299,\n",
       "  115,\n",
       "  206,\n",
       "  22,\n",
       "  167,\n",
       "  27,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [52,\n",
       "  11,\n",
       "  206,\n",
       "  337,\n",
       "  105,\n",
       "  167,\n",
       "  264,\n",
       "  233,\n",
       "  299,\n",
       "  115,\n",
       "  249,\n",
       "  91,\n",
       "  167,\n",
       "  298,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [219,\n",
       "  206,\n",
       "  337,\n",
       "  238,\n",
       "  167,\n",
       "  134,\n",
       "  233,\n",
       "  299,\n",
       "  115,\n",
       "  206,\n",
       "  337,\n",
       "  31,\n",
       "  167,\n",
       "  273,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [52,\n",
       "  11,\n",
       "  206,\n",
       "  292,\n",
       "  118,\n",
       "  167,\n",
       "  273,\n",
       "  233,\n",
       "  299,\n",
       "  115,\n",
       "  311,\n",
       "  105,\n",
       "  167,\n",
       "  208,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [287,\n",
       "  284,\n",
       "  140,\n",
       "  99,\n",
       "  206,\n",
       "  328,\n",
       "  150,\n",
       "  233,\n",
       "  149,\n",
       "  236,\n",
       "  284,\n",
       "  140,\n",
       "  206,\n",
       "  253,\n",
       "  221,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [26,\n",
       "  99,\n",
       "  258,\n",
       "  206,\n",
       "  204,\n",
       "  233,\n",
       "  149,\n",
       "  236,\n",
       "  258,\n",
       "  206,\n",
       "  328,\n",
       "  150,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [49,\n",
       "  206,\n",
       "  319,\n",
       "  167,\n",
       "  281,\n",
       "  233,\n",
       "  149,\n",
       "  115,\n",
       "  206,\n",
       "  337,\n",
       "  105,\n",
       "  167,\n",
       "  264,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [327,\n",
       "  248,\n",
       "  206,\n",
       "  50,\n",
       "  73,\n",
       "  176,\n",
       "  233,\n",
       "  299,\n",
       "  115,\n",
       "  206,\n",
       "  178,\n",
       "  30,\n",
       "  167,\n",
       "  134,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [338,\n",
       "  99,\n",
       "  206,\n",
       "  284,\n",
       "  140,\n",
       "  328,\n",
       "  122,\n",
       "  233,\n",
       "  149,\n",
       "  236,\n",
       "  284,\n",
       "  140,\n",
       "  206,\n",
       "  328,\n",
       "  150,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354],\n",
       " [52,\n",
       "  11,\n",
       "  206,\n",
       "  292,\n",
       "  50,\n",
       "  167,\n",
       "  230,\n",
       "  233,\n",
       "  299,\n",
       "  115,\n",
       "  206,\n",
       "  292,\n",
       "  31,\n",
       "  167,\n",
       "  298,\n",
       "  57,\n",
       "  357,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354,\n",
       "  354]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_text_to_int)\n",
    "len(target_text_to_int)\n",
    "source_text_to_int[:10]\n",
    "target_text_to_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建模型输入\n",
    "def get_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.placeholder(tf.int32, (None,), name='max_target_sequence_length')\n",
    "    return inputs, targets, learning_rate, source_sequence_length, target_sequence_length, max_target_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造encoder\n",
    "def encoder(input_data, rnn_size, layer_nums, \n",
    "           source_sequence_length, source_vocab_size, encoder_embedding_dim=100):\n",
    "    encoder_embeded = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoder_embedding_dim)\n",
    "    \n",
    "    def get_lstm_cell(rnn_size):\n",
    "        return tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2018))\n",
    "    multi_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(layer_nums)])\n",
    "    \n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(multi_cell, encoder_embeded, \n",
    "                                                      source_sequence_length, dtype=tf.float32)\n",
    "    return encoder_output, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer_input(target_input, target_vocab_to_int, batch_size):\n",
    "    ending = tf.strided_slice(target_input, [0,0], [batch_size, -1], [1,1])\n",
    "    decoder_input = tf.concat([tf.fill([batch_size,1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_train(encoding_states, decoder_cell, decoder_embeded,\n",
    "                 target_sequence_length, max_target_sequence_length, output_layer):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embeded, \n",
    "                                                        sequence_length=target_sequence_length,time_major=False)\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=training_helper, \n",
    "                                                       initial_state=encoding_states, output_layer=output_layer)\n",
    "    training_decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=training_decoder, \n",
    "                                                impute_finished=True, maximum_iterations=max_target_sequence_length)\n",
    "    return training_decoder_outputs\n",
    "\n",
    "def decoder_predict(encoding_states, decoder_cell, decoder_embeded, start_id, end_id, batch_size, \n",
    "                    target_sequence_length, max_target_sequence_length, output_layer):\n",
    "    start_tokens = tf.tile(tf.constant([start_id], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=decoder_embeded, \n",
    "                                                                start_tokens=start_tokens, end_token=end_id)\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=inference_helper, \n",
    "                                                        initial_state=encoding_states, output_layer=output_layer)\n",
    "    inference_decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=inference_decoder, \n",
    "                                                                        impute_finished=True, \n",
    "                                                                        maximum_iterations=max_target_sequence_length)\n",
    "    return inference_decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(decoder_input, encoder_states, rnn_size, layer_nums,\n",
    "            target_sequence_length, max_target_sequence_length, \n",
    "            target_vocab_size, decoder_embedding_dim, target_vocab_to_int, batch_size):\n",
    "    #embedding\n",
    "    target_embedding = tf.Variable(tf.random_uniform([target_vocab_size, decoder_embedding_dim]))\n",
    "    decoder_embeded = tf.nn.embedding_lookup(target_embedding, decoder_input)\n",
    "    \n",
    "    #lstm\n",
    "    def get_lstm_cell(rnn_size):\n",
    "        return tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2018))\n",
    "    multi_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(layer_nums)])\n",
    "    \n",
    "    #输出每一个词语的概率值\n",
    "    output_layer = tf.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    with tf.variable_scope('decoder'):\n",
    "        training_logits = decoder_train(encoder_states, multi_cell, \n",
    "                                                 decoder_embeded, target_sequence_length, \n",
    "                                                 max_target_sequence_length, output_layer)\n",
    "\n",
    "    with tf.variable_scope('decoder', reuse=True):\n",
    "        inference_logits = decoder_predict(encoder_states, multi_cell, target_embedding, \n",
    "                                                    target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], \n",
    "                                                    batch_size, target_sequence_length, max_target_sequence_length,\n",
    "                                                    output_layer)\n",
    "    return training_logits, inference_logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, rnn_size, layer_num, \n",
    "                 source_sequence_length, target_sequence_length, \n",
    "                 source_vocab_size, target_vocab_size, \n",
    "                 target_vocab_to_int, batch_size,\n",
    "                 encoder_embedding_dim, decoder_embedding_dim, max_target_sequence_length):\n",
    "    encoder_output, encoder_states = encoder(input_data, rnn_size, layer_nums, \n",
    "                                             source_sequence_length, source_vocab_size, encoder_embedding_dim)\n",
    "    \n",
    "    decoder_input = decoder_layer_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    training_decoder_outputs, inference_decoder_outputs = decoder(decoder_input, encoder_states, rnn_size, layer_nums, \n",
    "                                                                 target_sequence_length, max_target_sequence_length,\n",
    "                                                                 target_vocab_size, decoder_embedding_dim, \n",
    "                                                                 target_vocab_to_int, batch_size)\n",
    "    return training_decoder_outputs, inference_decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "rnn_size = 128\n",
    "layer_nums = 1\n",
    "encoder_embedding_dim = 100\n",
    "decoder_embedding_dim = 100\n",
    "learning_rate = 0.001\n",
    "display_step = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建图\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs, targets, lr, source_sequence_length, target_sequence_length, _ = get_inputs()\n",
    "    \n",
    "    max_target_sequence_length = 25\n",
    "    \n",
    "    training_decoder_outputs, inference_decoder_outputs = seq2seq_model(inputs, targets, rnn_size, layer_nums,\n",
    "                                                                source_sequence_length, target_sequence_length,\n",
    "                                                                len(source_word_to_int), len(target_word_to_int),\n",
    "                                                                target_word_to_int, batch_size, \n",
    "                                                                encoder_embedding_dim, decoder_embedding_dim,\n",
    "                                                                max_target_sequence_length)\n",
    "    training_logits = tf.identity(training_decoder_outputs.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_decoder_outputs.sample_id, name='predictions')\n",
    "    \n",
    "    sequence_mask = tf.sequence_mask(target_sequence_length, \n",
    "                                     max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope('optimization'):\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(training_logits, targets, sequence_mask)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        \n",
    "        gradients = optimizer.compute_gradients(loss)\n",
    "        clipped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        training_op = optimizer.apply_gradients(clipped_gradients)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(sources, targets, batch_size):\n",
    "    for batch_i in range(len(sources)//batch_size):\n",
    "        start = batch_i * batch_size\n",
    "        source_batch = sources[start:start+batch_size]\n",
    "        target_batch = targets[start:start+batch_size]\n",
    "        \n",
    "        target_lengths = []\n",
    "        for target in target_batch:\n",
    "            target_lengths.append(len(target))\n",
    "        \n",
    "        source_lengths = []\n",
    "        for source in source_batch:\n",
    "            source_lengths.append(len(source))\n",
    "        \n",
    "        yield source_batch, target_batch, source_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 50 / 1076 - train loss : 2.485210657119751 valid loss [2.404355]\n",
      "Epoch 0 Batch 100 / 1076 - train loss : 2.124098777770996 valid loss [2.0634453]\n",
      "Epoch 0 Batch 150 / 1076 - train loss : 1.7109979391098022 valid loss [1.6572142]\n",
      "Epoch 0 Batch 200 / 1076 - train loss : 1.3446513414382935 valid loss [1.3093833]\n",
      "Epoch 0 Batch 250 / 1076 - train loss : 1.0928196907043457 valid loss [1.077041]\n",
      "Epoch 0 Batch 300 / 1076 - train loss : 0.9008116126060486 valid loss [0.9304392]\n",
      "Epoch 0 Batch 350 / 1076 - train loss : 0.8312863707542419 valid loss [0.8304022]\n",
      "Epoch 0 Batch 400 / 1076 - train loss : 0.7913984656333923 valid loss [0.7604654]\n",
      "Epoch 0 Batch 450 / 1076 - train loss : 0.7378848195075989 valid loss [0.7096643]\n",
      "Epoch 0 Batch 500 / 1076 - train loss : 0.6618960499763489 valid loss [0.67250735]\n",
      "Epoch 0 Batch 550 / 1076 - train loss : 0.6739674210548401 valid loss [0.6434116]\n",
      "Epoch 0 Batch 600 / 1076 - train loss : 0.6489595174789429 valid loss [0.6128733]\n",
      "Epoch 0 Batch 650 / 1076 - train loss : 0.6018031239509583 valid loss [0.5881379]\n",
      "Epoch 0 Batch 700 / 1076 - train loss : 0.557970404624939 valid loss [0.565348]\n",
      "Epoch 0 Batch 750 / 1076 - train loss : 0.550948977470398 valid loss [0.54544634]\n",
      "Epoch 0 Batch 800 / 1076 - train loss : 0.5571685433387756 valid loss [0.5273521]\n",
      "Epoch 0 Batch 850 / 1076 - train loss : 0.4859977662563324 valid loss [0.5139139]\n",
      "Epoch 0 Batch 900 / 1076 - train loss : 0.5063847899436951 valid loss [0.50177306]\n",
      "Epoch 0 Batch 950 / 1076 - train loss : 0.4642268419265747 valid loss [0.489842]\n",
      "Epoch 0 Batch 1000 / 1076 - train loss : 0.4808749258518219 valid loss [0.47856835]\n",
      "Epoch 0 Batch 1050 / 1076 - train loss : 0.48405221104621887 valid loss [0.46835583]\n",
      "Epoch 1 Batch 50 / 1076 - train loss : 0.4688771069049835 valid loss [0.4560161]\n",
      "Epoch 1 Batch 100 / 1076 - train loss : 0.45275986194610596 valid loss [0.4421947]\n",
      "Epoch 1 Batch 150 / 1076 - train loss : 0.4220416247844696 valid loss [0.43509975]\n",
      "Epoch 1 Batch 200 / 1076 - train loss : 0.4007362127304077 valid loss [0.4268231]\n",
      "Epoch 1 Batch 250 / 1076 - train loss : 0.3964083194732666 valid loss [0.41694543]\n",
      "Epoch 1 Batch 300 / 1076 - train loss : 0.3964352309703827 valid loss [0.40404892]\n",
      "Epoch 1 Batch 350 / 1076 - train loss : 0.3855069875717163 valid loss [0.39848137]\n",
      "Epoch 1 Batch 400 / 1076 - train loss : 0.37983107566833496 valid loss [0.3853194]\n",
      "Epoch 1 Batch 450 / 1076 - train loss : 0.3892781138420105 valid loss [0.37991676]\n",
      "Epoch 1 Batch 500 / 1076 - train loss : 0.3679402470588684 valid loss [0.37286076]\n",
      "Epoch 1 Batch 550 / 1076 - train loss : 0.37396103143692017 valid loss [0.35806856]\n",
      "Epoch 1 Batch 600 / 1076 - train loss : 0.3660415709018707 valid loss [0.3527237]\n",
      "Epoch 1 Batch 650 / 1076 - train loss : 0.3379095494747162 valid loss [0.33680123]\n",
      "Epoch 1 Batch 700 / 1076 - train loss : 0.3192349672317505 valid loss [0.33419624]\n",
      "Epoch 1 Batch 750 / 1076 - train loss : 0.3149498701095581 valid loss [0.31610888]\n",
      "Epoch 1 Batch 800 / 1076 - train loss : 0.3180775046348572 valid loss [0.30684286]\n",
      "Epoch 1 Batch 850 / 1076 - train loss : 0.27933380007743835 valid loss [0.2940405]\n",
      "Epoch 1 Batch 900 / 1076 - train loss : 0.2835599482059479 valid loss [0.2891791]\n",
      "Epoch 1 Batch 950 / 1076 - train loss : 0.24926334619522095 valid loss [0.26957282]\n",
      "Epoch 1 Batch 1000 / 1076 - train loss : 0.25995635986328125 valid loss [0.25843227]\n",
      "Epoch 1 Batch 1050 / 1076 - train loss : 0.24765251576900482 valid loss [0.24463111]\n",
      "Epoch 2 Batch 50 / 1076 - train loss : 0.23740166425704956 valid loss [0.22894184]\n",
      "Epoch 2 Batch 100 / 1076 - train loss : 0.2219158113002777 valid loss [0.21565296]\n",
      "Epoch 2 Batch 150 / 1076 - train loss : 0.2053270787000656 valid loss [0.2120103]\n",
      "Epoch 2 Batch 200 / 1076 - train loss : 0.18604053556919098 valid loss [0.20192318]\n",
      "Epoch 2 Batch 250 / 1076 - train loss : 0.18970580399036407 valid loss [0.19389684]\n",
      "Epoch 2 Batch 300 / 1076 - train loss : 0.18523119390010834 valid loss [0.18052812]\n",
      "Epoch 2 Batch 350 / 1076 - train loss : 0.16911998391151428 valid loss [0.17120561]\n",
      "Epoch 2 Batch 400 / 1076 - train loss : 0.17223118245601654 valid loss [0.16400148]\n",
      "Epoch 2 Batch 450 / 1076 - train loss : 0.16735458374023438 valid loss [0.1563994]\n",
      "Epoch 2 Batch 500 / 1076 - train loss : 0.1435593068599701 valid loss [0.14497158]\n",
      "Epoch 2 Batch 550 / 1076 - train loss : 0.16224315762519836 valid loss [0.13901933]\n",
      "Epoch 2 Batch 600 / 1076 - train loss : 0.1516781598329544 valid loss [0.12901236]\n",
      "Epoch 2 Batch 650 / 1076 - train loss : 0.13545621931552887 valid loss [0.12578836]\n",
      "Epoch 2 Batch 700 / 1076 - train loss : 0.11402803659439087 valid loss [0.11735978]\n",
      "Epoch 2 Batch 750 / 1076 - train loss : 0.11654851585626602 valid loss [0.11292788]\n",
      "Epoch 2 Batch 800 / 1076 - train loss : 0.12687426805496216 valid loss [0.10885035]\n",
      "Epoch 2 Batch 850 / 1076 - train loss : 0.09759515523910522 valid loss [0.10118257]\n",
      "Epoch 2 Batch 900 / 1076 - train loss : 0.09721493721008301 valid loss [0.094819635]\n",
      "Epoch 2 Batch 950 / 1076 - train loss : 0.09090414643287659 valid loss [0.09076504]\n",
      "Epoch 2 Batch 1000 / 1076 - train loss : 0.09565681219100952 valid loss [0.087982215]\n",
      "Epoch 2 Batch 1050 / 1076 - train loss : 0.08967811614274979 valid loss [0.084632464]\n",
      "Epoch 3 Batch 50 / 1076 - train loss : 0.08984813839197159 valid loss [0.07532563]\n",
      "Epoch 3 Batch 100 / 1076 - train loss : 0.0801512598991394 valid loss [0.074591115]\n",
      "Epoch 3 Batch 150 / 1076 - train loss : 0.07805337756872177 valid loss [0.07302622]\n",
      "Epoch 3 Batch 200 / 1076 - train loss : 0.06588669866323471 valid loss [0.06893424]\n",
      "Epoch 3 Batch 250 / 1076 - train loss : 0.07269717007875443 valid loss [0.06649889]\n",
      "Epoch 3 Batch 300 / 1076 - train loss : 0.07013826072216034 valid loss [0.06346776]\n",
      "Epoch 3 Batch 350 / 1076 - train loss : 0.06510192900896072 valid loss [0.062126756]\n",
      "Epoch 3 Batch 400 / 1076 - train loss : 0.06489484012126923 valid loss [0.06160958]\n",
      "Epoch 3 Batch 450 / 1076 - train loss : 0.06674046814441681 valid loss [0.06087763]\n",
      "Epoch 3 Batch 500 / 1076 - train loss : 0.05504276230931282 valid loss [0.05759696]\n",
      "Epoch 3 Batch 550 / 1076 - train loss : 0.0733206495642662 valid loss [0.05837177]\n",
      "Epoch 3 Batch 600 / 1076 - train loss : 0.0709262490272522 valid loss [0.052748375]\n",
      "Epoch 3 Batch 650 / 1076 - train loss : 0.059293948113918304 valid loss [0.04979583]\n",
      "Epoch 3 Batch 700 / 1076 - train loss : 0.046758051961660385 valid loss [0.049817972]\n",
      "Epoch 3 Batch 750 / 1076 - train loss : 0.05537028610706329 valid loss [0.04828336]\n",
      "Epoch 3 Batch 800 / 1076 - train loss : 0.06038377806544304 valid loss [0.048891272]\n",
      "Epoch 3 Batch 850 / 1076 - train loss : 0.04431276023387909 valid loss [0.044418916]\n",
      "Epoch 3 Batch 900 / 1076 - train loss : 0.049851126968860626 valid loss [0.042978]\n",
      "Epoch 3 Batch 950 / 1076 - train loss : 0.041956137865781784 valid loss [0.042788964]\n",
      "Epoch 3 Batch 1000 / 1076 - train loss : 0.052329983562231064 valid loss [0.046012577]\n",
      "Epoch 3 Batch 1050 / 1076 - train loss : 0.04666834697127342 valid loss [0.040580865]\n",
      "Epoch 4 Batch 50 / 1076 - train loss : 0.050488851964473724 valid loss [0.03938453]\n",
      "Epoch 4 Batch 100 / 1076 - train loss : 0.042233265936374664 valid loss [0.038612135]\n",
      "Epoch 4 Batch 150 / 1076 - train loss : 0.04470061883330345 valid loss [0.04151227]\n",
      "Epoch 4 Batch 200 / 1076 - train loss : 0.03693659231066704 valid loss [0.03729299]\n",
      "Epoch 4 Batch 250 / 1076 - train loss : 0.04143862426280975 valid loss [0.03525732]\n",
      "Epoch 4 Batch 300 / 1076 - train loss : 0.041674137115478516 valid loss [0.038552612]\n",
      "Epoch 4 Batch 350 / 1076 - train loss : 0.03985614702105522 valid loss [0.03538111]\n",
      "Epoch 4 Batch 400 / 1076 - train loss : 0.04032301530241966 valid loss [0.035925146]\n",
      "Epoch 4 Batch 450 / 1076 - train loss : 0.03692946583032608 valid loss [0.03485063]\n",
      "Epoch 4 Batch 500 / 1076 - train loss : 0.03321005031466484 valid loss [0.034023475]\n",
      "Epoch 4 Batch 550 / 1076 - train loss : 0.046991243958473206 valid loss [0.03789092]\n",
      "Epoch 4 Batch 600 / 1076 - train loss : 0.046667952090501785 valid loss [0.031958643]\n",
      "Epoch 4 Batch 650 / 1076 - train loss : 0.037046145647764206 valid loss [0.032007433]\n",
      "Epoch 4 Batch 700 / 1076 - train loss : 0.028799891471862793 valid loss [0.031946555]\n",
      "Epoch 4 Batch 750 / 1076 - train loss : 0.03361360728740692 valid loss [0.03210158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 800 / 1076 - train loss : 0.04452865943312645 valid loss [0.031018548]\n",
      "Epoch 4 Batch 850 / 1076 - train loss : 0.03118615224957466 valid loss [0.030008055]\n",
      "Epoch 4 Batch 900 / 1076 - train loss : 0.033441413193941116 valid loss [0.027534194]\n",
      "Epoch 4 Batch 950 / 1076 - train loss : 0.026513107120990753 valid loss [0.027599463]\n",
      "Epoch 4 Batch 1000 / 1076 - train loss : 0.035255495458841324 valid loss [0.028992085]\n",
      "Epoch 4 Batch 1050 / 1076 - train loss : 0.03372083231806755 valid loss [0.026985746]\n",
      "Epoch 5 Batch 50 / 1076 - train loss : 0.03709790110588074 valid loss [0.025753718]\n",
      "Epoch 5 Batch 100 / 1076 - train loss : 0.02853628620505333 valid loss [0.02494411]\n",
      "Epoch 5 Batch 150 / 1076 - train loss : 0.029996275901794434 valid loss [0.02625951]\n",
      "Epoch 5 Batch 200 / 1076 - train loss : 0.025507405400276184 valid loss [0.024797846]\n",
      "Epoch 5 Batch 250 / 1076 - train loss : 0.030301712453365326 valid loss [0.023755211]\n",
      "Epoch 5 Batch 300 / 1076 - train loss : 0.02936396561563015 valid loss [0.026151374]\n",
      "Epoch 5 Batch 350 / 1076 - train loss : 0.027601853013038635 valid loss [0.025329275]\n",
      "Epoch 5 Batch 400 / 1076 - train loss : 0.027501091361045837 valid loss [0.024163121]\n",
      "Epoch 5 Batch 450 / 1076 - train loss : 0.02625599317252636 valid loss [0.024844175]\n",
      "Epoch 5 Batch 500 / 1076 - train loss : 0.025134552270174026 valid loss [0.024727946]\n",
      "Epoch 5 Batch 550 / 1076 - train loss : 0.03465564176440239 valid loss [0.026323983]\n",
      "Epoch 5 Batch 600 / 1076 - train loss : 0.03484032303094864 valid loss [0.02503308]\n",
      "Epoch 5 Batch 650 / 1076 - train loss : 0.02601582556962967 valid loss [0.022710789]\n",
      "Epoch 5 Batch 700 / 1076 - train loss : 0.020926523953676224 valid loss [0.022659615]\n",
      "Epoch 5 Batch 750 / 1076 - train loss : 0.02194661647081375 valid loss [0.0243891]\n",
      "Epoch 5 Batch 800 / 1076 - train loss : 0.03189196437597275 valid loss [0.021442832]\n",
      "Epoch 5 Batch 850 / 1076 - train loss : 0.023007767274975777 valid loss [0.02339658]\n",
      "Epoch 5 Batch 900 / 1076 - train loss : 0.027697838842868805 valid loss [0.02339994]\n",
      "Epoch 5 Batch 950 / 1076 - train loss : 0.01947452500462532 valid loss [0.021889383]\n",
      "Epoch 5 Batch 1000 / 1076 - train loss : 0.027030853554606438 valid loss [0.021054454]\n",
      "Epoch 5 Batch 1050 / 1076 - train loss : 0.025437919422984123 valid loss [0.02153849]\n",
      "Epoch 6 Batch 50 / 1076 - train loss : 0.029980702325701714 valid loss [0.02030393]\n",
      "Epoch 6 Batch 100 / 1076 - train loss : 0.02199224941432476 valid loss [0.018554213]\n",
      "Epoch 6 Batch 150 / 1076 - train loss : 0.023923087865114212 valid loss [0.021243978]\n",
      "Epoch 6 Batch 200 / 1076 - train loss : 0.019745662808418274 valid loss [0.019616887]\n",
      "Epoch 6 Batch 250 / 1076 - train loss : 0.021107995882630348 valid loss [0.018614622]\n",
      "Epoch 6 Batch 300 / 1076 - train loss : 0.023646987974643707 valid loss [0.019789778]\n",
      "Epoch 6 Batch 350 / 1076 - train loss : 0.021360285580158234 valid loss [0.019973963]\n",
      "Epoch 6 Batch 400 / 1076 - train loss : 0.020887989550828934 valid loss [0.019396856]\n",
      "Epoch 6 Batch 450 / 1076 - train loss : 0.02066176012158394 valid loss [0.01868539]\n",
      "Epoch 6 Batch 500 / 1076 - train loss : 0.02108723111450672 valid loss [0.019506622]\n",
      "Epoch 6 Batch 550 / 1076 - train loss : 0.026583323255181313 valid loss [0.021219084]\n",
      "Epoch 6 Batch 600 / 1076 - train loss : 0.02811248041689396 valid loss [0.020529894]\n",
      "Epoch 6 Batch 650 / 1076 - train loss : 0.019895877689123154 valid loss [0.017757678]\n",
      "Epoch 6 Batch 700 / 1076 - train loss : 0.017553897574543953 valid loss [0.018270124]\n",
      "Epoch 6 Batch 750 / 1076 - train loss : 0.01711181364953518 valid loss [0.022729943]\n",
      "Epoch 6 Batch 800 / 1076 - train loss : 0.025042781606316566 valid loss [0.018413663]\n",
      "Epoch 6 Batch 850 / 1076 - train loss : 0.018244318664073944 valid loss [0.017832449]\n",
      "Epoch 6 Batch 900 / 1076 - train loss : 0.02211456373333931 valid loss [0.017737268]\n",
      "Epoch 6 Batch 950 / 1076 - train loss : 0.015233605168759823 valid loss [0.016679203]\n",
      "Epoch 6 Batch 1000 / 1076 - train loss : 0.022017966955900192 valid loss [0.017353388]\n",
      "Epoch 6 Batch 1050 / 1076 - train loss : 0.02075335755944252 valid loss [0.019179814]\n",
      "Epoch 7 Batch 50 / 1076 - train loss : 0.02450995147228241 valid loss [0.016769597]\n",
      "Epoch 7 Batch 100 / 1076 - train loss : 0.018671967089176178 valid loss [0.01780911]\n",
      "Epoch 7 Batch 150 / 1076 - train loss : 0.016862625256180763 valid loss [0.01851543]\n",
      "Epoch 7 Batch 200 / 1076 - train loss : 0.01577935181558132 valid loss [0.015782248]\n",
      "Epoch 7 Batch 250 / 1076 - train loss : 0.016029316931962967 valid loss [0.016246466]\n",
      "Epoch 7 Batch 300 / 1076 - train loss : 0.01995769515633583 valid loss [0.016552009]\n",
      "Epoch 7 Batch 350 / 1076 - train loss : 0.017971061170101166 valid loss [0.02002372]\n",
      "Epoch 7 Batch 400 / 1076 - train loss : 0.016375040635466576 valid loss [0.015613045]\n",
      "Epoch 7 Batch 450 / 1076 - train loss : 0.01695225201547146 valid loss [0.017046811]\n",
      "Epoch 7 Batch 500 / 1076 - train loss : 0.016369374468922615 valid loss [0.01666689]\n",
      "Epoch 7 Batch 550 / 1076 - train loss : 0.021621540188789368 valid loss [0.017950071]\n",
      "Epoch 7 Batch 600 / 1076 - train loss : 0.0233288761228323 valid loss [0.015276885]\n",
      "Epoch 7 Batch 650 / 1076 - train loss : 0.01510374341160059 valid loss [0.014635286]\n",
      "Epoch 7 Batch 700 / 1076 - train loss : 0.01369586680084467 valid loss [0.013427154]\n",
      "Epoch 7 Batch 750 / 1076 - train loss : 0.014728260226547718 valid loss [0.01963413]\n",
      "Epoch 7 Batch 800 / 1076 - train loss : 0.01981857605278492 valid loss [0.0148842195]\n",
      "Epoch 7 Batch 850 / 1076 - train loss : 0.014643821865320206 valid loss [0.015877113]\n",
      "Epoch 7 Batch 900 / 1076 - train loss : 0.01828867755830288 valid loss [0.015887372]\n",
      "Epoch 7 Batch 950 / 1076 - train loss : 0.012391157448291779 valid loss [0.01425667]\n",
      "Epoch 7 Batch 1000 / 1076 - train loss : 0.018217680975794792 valid loss [0.015761442]\n",
      "Epoch 7 Batch 1050 / 1076 - train loss : 0.017436500638723373 valid loss [0.014447562]\n",
      "Epoch 8 Batch 50 / 1076 - train loss : 0.021100126206874847 valid loss [0.013156999]\n",
      "Epoch 8 Batch 100 / 1076 - train loss : 0.018970470875501633 valid loss [0.017739713]\n",
      "Epoch 8 Batch 150 / 1076 - train loss : 0.013440148904919624 valid loss [0.0148026105]\n",
      "Epoch 8 Batch 200 / 1076 - train loss : 0.012546238489449024 valid loss [0.012974832]\n",
      "Epoch 8 Batch 250 / 1076 - train loss : 0.014259972609579563 valid loss [0.014253833]\n",
      "Epoch 8 Batch 300 / 1076 - train loss : 0.0182859618216753 valid loss [0.013163602]\n",
      "Epoch 8 Batch 350 / 1076 - train loss : 0.013609800487756729 valid loss [0.015282142]\n",
      "Epoch 8 Batch 400 / 1076 - train loss : 0.013522416353225708 valid loss [0.013373697]\n",
      "Epoch 8 Batch 450 / 1076 - train loss : 0.014143171720206738 valid loss [0.014684613]\n",
      "Epoch 8 Batch 500 / 1076 - train loss : 0.014841411262750626 valid loss [0.016276712]\n",
      "Epoch 8 Batch 550 / 1076 - train loss : 0.017611775547266006 valid loss [0.016299386]\n",
      "Epoch 8 Batch 600 / 1076 - train loss : 0.019781414419412613 valid loss [0.014118094]\n",
      "Epoch 8 Batch 650 / 1076 - train loss : 0.012948162853717804 valid loss [0.011202757]\n",
      "Epoch 8 Batch 700 / 1076 - train loss : 0.010796863585710526 valid loss [0.012295778]\n",
      "Epoch 8 Batch 750 / 1076 - train loss : 0.011828554794192314 valid loss [0.017668862]\n",
      "Epoch 8 Batch 800 / 1076 - train loss : 0.0169074647128582 valid loss [0.012168323]\n",
      "Epoch 8 Batch 850 / 1076 - train loss : 0.011306597851216793 valid loss [0.014458869]\n",
      "Epoch 8 Batch 900 / 1076 - train loss : 0.015827471390366554 valid loss [0.014135497]\n",
      "Epoch 8 Batch 950 / 1076 - train loss : 0.011309930123388767 valid loss [0.012935512]\n",
      "Epoch 8 Batch 1000 / 1076 - train loss : 0.01837846450507641 valid loss [0.015849609]\n",
      "Epoch 8 Batch 1050 / 1076 - train loss : 0.01477417629212141 valid loss [0.014391296]\n",
      "Epoch 9 Batch 50 / 1076 - train loss : 0.018777692690491676 valid loss [0.012110723]\n",
      "Epoch 9 Batch 100 / 1076 - train loss : 0.014595887623727322 valid loss [0.011755219]\n",
      "Epoch 9 Batch 150 / 1076 - train loss : 0.010171961970627308 valid loss [0.011442405]\n",
      "Epoch 9 Batch 200 / 1076 - train loss : 0.011024919338524342 valid loss [0.011373014]\n",
      "Epoch 9 Batch 250 / 1076 - train loss : 0.010862371884286404 valid loss [0.013653989]\n",
      "Epoch 9 Batch 300 / 1076 - train loss : 0.014639492146670818 valid loss [0.01215375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 350 / 1076 - train loss : 0.01210806518793106 valid loss [0.012726715]\n",
      "Epoch 9 Batch 400 / 1076 - train loss : 0.012467622756958008 valid loss [0.012413729]\n",
      "Epoch 9 Batch 450 / 1076 - train loss : 0.011604207567870617 valid loss [0.012991531]\n",
      "Epoch 9 Batch 500 / 1076 - train loss : 0.012234819121658802 valid loss [0.014304447]\n",
      "Epoch 9 Batch 550 / 1076 - train loss : 0.01606171578168869 valid loss [0.0139122475]\n",
      "Epoch 9 Batch 600 / 1076 - train loss : 0.01661423221230507 valid loss [0.013314065]\n",
      "Epoch 9 Batch 650 / 1076 - train loss : 0.010615145787596703 valid loss [0.011320461]\n",
      "Epoch 9 Batch 700 / 1076 - train loss : 0.00906676147133112 valid loss [0.0116267875]\n",
      "Epoch 9 Batch 750 / 1076 - train loss : 0.0097084054723382 valid loss [0.013924808]\n",
      "Epoch 9 Batch 800 / 1076 - train loss : 0.014812331646680832 valid loss [0.011212528]\n",
      "Epoch 9 Batch 850 / 1076 - train loss : 0.009415068663656712 valid loss [0.012150572]\n",
      "Epoch 9 Batch 900 / 1076 - train loss : 0.014061298221349716 valid loss [0.014181027]\n",
      "Epoch 9 Batch 950 / 1076 - train loss : 0.00849432684481144 valid loss [0.011970218]\n",
      "Epoch 9 Batch 1000 / 1076 - train loss : 0.016348712146282196 valid loss [0.01459264]\n",
      "Epoch 9 Batch 1050 / 1076 - train loss : 0.012424003332853317 valid loss [0.012262042]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model/checkpoints'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source = source_text_to_int[batch_size:]\n",
    "train_target = target_text_to_int[batch_size:]\n",
    "\n",
    "valid_source = source_text_to_int[:batch_size]\n",
    "valid_target = target_text_to_int[:batch_size]\n",
    "(valid_source_batch, valid_target_batch, valid_source_lengths, valid_target_lengths) = next(get_batches(\n",
    "    valid_source, valid_target, batch_size))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, source_lengths, target_lengths) in enumerate(\n",
    "            get_batches(train_source, train_target, batch_size)):\n",
    "            _, train_loss = sess.run([training_op, loss], feed_dict={\n",
    "                inputs:source_batch,\n",
    "                targets:target_batch,\n",
    "                lr:learning_rate,\n",
    "                source_sequence_length:source_lengths,\n",
    "                target_sequence_length:target_lengths\n",
    "            })\n",
    "            if (batch_i+1) % display_step == 0:\n",
    "                valid_loss = sess.run([loss], feed_dict={\n",
    "                    inputs:valid_source_batch,\n",
    "                    targets:valid_target_batch,\n",
    "                    lr:learning_rate,\n",
    "                    source_sequence_length:valid_source_lengths,\n",
    "                    target_sequence_length:valid_target_lengths\n",
    "                })\n",
    "                print('Epoch {} Batch {} / {} - train loss : {} valid loss {}'.format(\n",
    "                    epoch, batch_i+1, len(train_source) // batch_size, train_loss, valid_loss))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, 'model/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lvzcl/Desktop/ml_dl/rnn/seq2seq_translation\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_seq(sentence, source_vocab_to_int):\n",
    "    unk_idx = source_vocab_to_int[\"<UNK>\"]\n",
    "    word_idx = [source_vocab_to_int.get(word, unk_idx) for word in sentence.lower().split()]\n",
    "    return word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/checkpoints\n",
      "【Input】\n",
      "  Word Ids:      [98, 68, 43, 166, 24, 166, 156, 141, 160]\n",
      "  English Words: ['i', 'dislike', 'grapefruit', ',', 'lemons', ',', 'and', 'peaches', '.']\n",
      "\n",
      "【Prediction】\n",
      "  Word Ids:      [239, 54, 133, 233, 299, 52, 352, 57, 357]\n",
      "  French Words: ['vous', \"n'aimez\", 'pamplemousses', ',', 'et', 'les', 'citrons', '.', '<EOS>']\n",
      "\n",
      "【Full Sentence】\n",
      "vous n'aimez pamplemousses , et les citrons . <EOS>\n"
     ]
    }
   ],
   "source": [
    "translate_sentence_text = 'i dislike grapefruit , lemons , and peaches .'\n",
    "target_text = valid_target[0]\n",
    "translate_sentence = sequence_to_seq(translate_sentence_text, source_word_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph = loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph('model/checkpoints.meta')\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "    \n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    \n",
    "    translate_logits = sess.run(logits, feed_dict={\n",
    "        input_data:[translate_sentence] * batch_size,\n",
    "        target_sequence_length:[len(translate_sentence)*2]*batch_size,\n",
    "        source_sequence_length:[len(translate_sentence)*2]*batch_size\n",
    "    })[0]\n",
    "\n",
    "print('【Input】')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_word[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\n【Prediction】')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format([target_int_to_word[i] for i in translate_logits]))\n",
    "\n",
    "print(\"\\n【Full Sentence】\")\n",
    "print(\" \".join([target_int_to_word[i] for i in translate_logits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
