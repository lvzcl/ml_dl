{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/small_vocab_en', 'r', encoding='utf-8') as f:\n",
    "    source_text = f.read()\n",
    "with open('./data/small_vocab_fr', 'r', encoding='utf-8') as f:\n",
    "    target_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stats\n",
      "Rough the number of unique words: 852\n",
      "----------English text----------\n",
      "number of sentences is: 137861\n",
      "Average number of word in sentence is: 13.225589543090503\n",
      "Max number of word in sentence is 17\n",
      "----------French text----------\n",
      "number of sentences is 137861\n",
      "Average number of word in sentence is 13.225589543090503\n",
      "Max number of word in sentence is 17\n",
      "English sentences from 0 to 10\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "French sentences from 0 to 10\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0,10)\n",
    "print('Data stats')\n",
    "print('Rough the number of unique words: {}'.format(len({word:None for word in source_text.split(' ')})))\n",
    "\n",
    "print('-'*10 + 'English text' + '-'*10)\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split(' ')) for sentence in sentences]\n",
    "print('number of sentences is: {}'.format(len(sentences)))\n",
    "print('Average number of word in sentence is: {}'.format(np.average(word_counts)))\n",
    "print('Max number of word in sentence is {}'.format(np.max(word_counts)))\n",
    "\n",
    "print('-'*10 + 'French text' + '-'*10)\n",
    "sentences = target_text.split('\\n')\n",
    "words_counts = [len(sentence) for sentence in sentences]\n",
    "print('number of sentences is {}'.format(len(sentences)))\n",
    "print('Average number of word in sentence is {}'.format(np.average(word_counts)))\n",
    "print('Max number of word in sentence is {}'.format(np.max(word_counts)))\n",
    "\n",
    "print('English sentences from {} to {}'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0] : view_sentence_range[1]]))\n",
    "\n",
    "print('French sentences from {} to {}'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0] : view_sentence_range[1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#之后需要构造映射\n",
    "#特殊字符\n",
    "source_codes = ['<PAD>', '<UNK>']\n",
    "target_codes = ['<GO>', '<PAD>', '<UNK>', '<EOS>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = list(set(source_text.lower().split()))\n",
    "target_vocab = list(set(target_text.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in source vocab : 227\n",
      "number of words in target vocab : 354\n"
     ]
    }
   ],
   "source": [
    "print('number of words in source vocab : {}'.format(len(source_vocab)))\n",
    "print('number of words in target vocab : {}'.format(len(target_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造字典\n",
    "source_vocab_to_int = {word : idx for idx, word in enumerate(source_codes + source_vocab)}\n",
    "source_int_to_vocab = {idx : word for idx, word in enumerate(source_codes + source_vocab)}\n",
    "\n",
    "target_vocab_to_int = {word : idx for idx, word in enumerate(target_codes + target_vocab)}\n",
    "target_int_to_vocab = {idx : word for idx, word in enumerate(target_codes + target_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of English Map is: 229\n",
      "The size of French Map is : 358\n"
     ]
    }
   ],
   "source": [
    "print('The size of English Map is: {}'.format(len(source_vocab_to_int)))\n",
    "print('The size of French Map is : {}'.format(len(target_vocab_to_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_int(sentence, map_dict, max_length=20, is_target=False):\n",
    "    text_to_idx = []\n",
    "    \n",
    "    unk_to_idx = map_dict.get('<UNK>')\n",
    "    pad_to_idx = map_dict.get('<PAD>')\n",
    "    eos_to_idx = map_dict.get('<EOS>')\n",
    "    \n",
    "    #如果是输入文本，直接将句子中的单词的idx加入list，如果不是，则在后面添加EOS\n",
    "    if not is_target:\n",
    "        for word in sentence.lower().split():\n",
    "            text_to_idx.append(map_dict.get(word, unk_to_idx))\n",
    "    else:\n",
    "        for word in sentence.lower().split():\n",
    "            text_to_idx.append(map_dict.get(word, unk_to_idx))\n",
    "        text_to_idx.append(eos_to_idx)\n",
    "    #超出长度需要截断，长度不够，进行填充\n",
    "    if len(text_to_idx) > max_length:\n",
    "        text_to_idx = text_to_idx[:max_length]\n",
    "    else:\n",
    "        text_to_idx = text_to_idx + [pad_to_idx] * (max_length - len(text_to_idx))\n",
    "    return text_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137861/137861 [00:01<00:00, 114051.30it/s]\n"
     ]
    }
   ],
   "source": [
    "source_text_to_int = []\n",
    "for sentence in tqdm.tqdm(source_text.split('\\n')):\n",
    "    source_text_to_int.append(text_to_int(sentence, source_vocab_to_int, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137861/137861 [00:01<00:00, 100096.71it/s]\n"
     ]
    }
   ],
   "source": [
    "target_text_to_int = []\n",
    "for sentence in tqdm.tqdm(target_text.split('\\n')):\n",
    "    target_text_to_int.append(text_to_int(sentence, target_vocab_to_int, 25, is_target=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----English-----\n",
      "the united states is never beautiful during march , and it is usually relaxing in summer .\n",
      "[170, 135, 63, 108, 62, 49, 33, 18, 145, 15, 12, 108, 168, 80, 165, 159, 160, 0, 0, 0]\n",
      "-----French-----\n",
      "les états-unis est jamais belle en mars , et il est relaxant habituellement en été .\n",
      "[306, 85, 91, 210, 47, 151, 28, 156, 197, 177, 91, 51, 201, 151, 147, 66, 3, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "random_index= 77\n",
    "print('-'*5 + 'English' + '-'*5)\n",
    "print(source_text.split('\\n')[random_index])\n",
    "print(source_text_to_int[random_index])\n",
    "\n",
    "print('-'*5 + 'French' + '-'*5)\n",
    "print(target_text.split('\\n')[random_index])\n",
    "print(target_text_to_int[random_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    #获取输入参数，分别为inputs, targets, learning_rate, max_target_length, source_sentence_length, target_sentence_length\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    source_sequence_length = tf.placeholder(tf.int32, [None,], name='source_sequence_length')\n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None,], name='target_sequence_length')\n",
    "    max_target_length = tf.placeholder(tf.int32, [None,], name='max_target_length')\n",
    "    \n",
    "    return inputs, targets, learning_rate, source_sequence_length, target_sequence_length, max_target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_layer(inputs, rnn_size, num_layers, source_sequence_length, source_vocab_size,encoder_embedding_size=100):\n",
    "    encoder_embed = tf.contrib.layers.embed_sequence(inputs, source_vocab_size, encoder_embedding_size)\n",
    "    \n",
    "    def get_lstm_cell(rnn_size):\n",
    "        return tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "    lstms = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])\n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(lstms, encoder_embed, source_sequence_length, dtype=tf.float32)\n",
    "    return encoder_output, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_layer_input(target_data, target_vocab_to_int, batch_size):\n",
    "    #删除最后一个字符，在第一个字符位置添加'<GO>'\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1,1])\n",
    "    decoder_inputs = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int[\"<GO>\"]), ending], 1)\n",
    "    return decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_layer_train(encoder_states, decoder_cell, decoder_embed, \n",
    "                        target_sequence_length, max_target_length, output_layer):\n",
    "    \"\"\"\n",
    "    Decoder端的训练\n",
    "    \n",
    "    @param encoder_states: Encoder端编码得到的Context Vector\n",
    "    @param decoder_cell: Decoder端\n",
    "    @param decoder_embed: Decoder端词向量嵌入后的输入\n",
    "    @param target_sequence_len: 法语文本的长度\n",
    "    @param max_target_sequence_len: 法语文本的最大长度\n",
    "    @param output_layer: 输出层\n",
    "    \"\"\"\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_embed, target_sequence_length, time_major=False)\n",
    "    \n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, training_helper, encoder_states, output_layer)\n",
    "    \n",
    "    training_outputs, _,  _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        training_decoder, impute_finished=True, maximum_iterations=max_target_length)\n",
    "    \n",
    "    return training_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer_infer(encoder_states, decoder_cell, decoder_embed, \n",
    "                        start_id, end_id, max_target_length, output_layer, batch_size):\n",
    "    \"\"\"\n",
    "    Decoder端的预测/推断\n",
    "    \n",
    "    @param encoder_states: Encoder端编码得到的Context Vector\n",
    "    @param decoder_cell: Decoder端\n",
    "    @param decoder_embed: Decoder端词向量嵌入后的输入\n",
    "    @param start_id: 句子起始单词的token id， 即\"<GO>\"的编码\n",
    "    @param end_id: 句子结束的token id，即\"<EOS>\"的编码\n",
    "    @param max_target_sequence_len: 法语文本的最大长度\n",
    "    @param output_layer: 输出层\n",
    "    @batch_size: batch size\n",
    "    \"\"\"\n",
    "    start_tokens = tf.tile(tf.constant([start_id],dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embed, start_tokens, end_id)\n",
    "    \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, inference_helper, encoder_states, output_layer)\n",
    "    \n",
    "    inference_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder, impute_finished=True, maximum_iterations=max_target_length)\n",
    "    \n",
    "    return inference_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_layer(encoder_states, decoder_inputs, rnn_size, num_layers,\n",
    "                 target_sequence_length, max_target_length, target_vocab_to_int,\n",
    "                  target_vocab_size, decoder_embedding_size, batch_size):\n",
    "    decoder_embedding = tf.Variable(tf.random_uniform([target_vocab_size, decoder_embedding_size]))\n",
    "    decoder_embed = tf.nn.embedding_lookup(decoder_embedding, decoder_inputs)\n",
    "    \n",
    "    def get_lstm(rnn_size):\n",
    "        return tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=456))\n",
    "    \n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    output_layer = tf.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    with tf.variable_scope('decoder'):\n",
    "        training_logits = decoder_layer_train(encoder_states, decoder_cell, \n",
    "                                              decoder_embed, target_sequence_length, max_target_length, output_layer)\n",
    "        \n",
    "    with tf.variable_scope('decoder', reuse=True):\n",
    "        inference_logits = decoder_layer_infer(encoder_states, decoder_cell, decoder_embedding,\n",
    "                                               target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'],\n",
    "                                               max_target_length, output_layer, batch_size)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, batch_size, \n",
    "                  source_sequence_length, target_sequence_length, max_target_length,\n",
    "                  source_vocab_size, target_vocab_size, encoder_embedding_size, decoder_embedding_size, \n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    encoder_output, encoder_state = encoder_layer(inputs, rnn_size, num_layers, source_sequence_length, \n",
    "                                                 source_vocab_size, encoder_embedding_size)\n",
    "    decoder_inputs = decoder_layer_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    training_logits, inference_logits = decoder_layer(encoder_state, decoder_inputs, rnn_size, num_layers,\n",
    "                                                     target_sequence_length, max_target_length, target_vocab_to_int,\n",
    "                                                     target_vocab_size, encoder_embedding_size, batch_size)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 1\n",
    "encoder_embedding_size = 100\n",
    "decoder_embedding_size = 100\n",
    "lr = 0.001\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    inputs, targets, learning_rate, source_sequence_length, target_sequence_length, max_target_length = get_inputs()\n",
    "    max_target_length = 25 \n",
    "    training_logits, inference_logits = seq2seq_model(inputs, targets, batch_size, source_sequence_length,\n",
    "                                                      target_sequence_length, max_target_length,\n",
    "                                                      len(source_vocab_to_int), len(target_vocab_to_int), \n",
    "                                                      encoder_embedding_size, decoder_embedding_size, rnn_size,\n",
    "                                                      num_layers, target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(training_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_length, dtype=tf.float32, name='mask')\n",
    "    \n",
    "    with tf.name_scope('optimization'):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        clipped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        training_op = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(sources, targets, batch_size):\n",
    "    for batch_i in range(0, len(sources) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        \n",
    "        source_batch = sources[start_i : start_i + batch_size]\n",
    "        target_batch = targets[start_i : start_i + batch_size]\n",
    "        \n",
    "        target_length = []\n",
    "        for target in target_batch:\n",
    "            target_length.append(len(target))\n",
    "            \n",
    "        source_length = []\n",
    "        for source in source_batch:\n",
    "            source_length.append(len(source))\n",
    "        yield source_batch, target_batch, source_length, target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   50/1077 - Loss: 2.5459\n",
      "Epoch   0 Batch  100/1077 - Loss: 2.1668\n",
      "Epoch   0 Batch  150/1077 - Loss: 1.7511\n",
      "Epoch   0 Batch  200/1077 - Loss: 1.3374\n",
      "Epoch   0 Batch  250/1077 - Loss: 1.0660\n",
      "Epoch   0 Batch  300/1077 - Loss: 0.8806\n",
      "Epoch   0 Batch  350/1077 - Loss: 0.8053\n",
      "Epoch   0 Batch  400/1077 - Loss: 0.7636\n",
      "Epoch   0 Batch  450/1077 - Loss: 0.7136\n",
      "Epoch   0 Batch  500/1077 - Loss: 0.6349\n",
      "Epoch   0 Batch  550/1077 - Loss: 0.6558\n",
      "Epoch   0 Batch  600/1077 - Loss: 0.6260\n",
      "Epoch   0 Batch  650/1077 - Loss: 0.5812\n",
      "Epoch   0 Batch  700/1077 - Loss: 0.5408\n",
      "Epoch   0 Batch  750/1077 - Loss: 0.5455\n",
      "Epoch   0 Batch  800/1077 - Loss: 0.5466\n",
      "Epoch   0 Batch  850/1077 - Loss: 0.4790\n",
      "Epoch   0 Batch  900/1077 - Loss: 0.4970\n",
      "Epoch   0 Batch  950/1077 - Loss: 0.4538\n",
      "Epoch   0 Batch 1000/1077 - Loss: 0.4640\n",
      "Epoch   0 Batch 1050/1077 - Loss: 0.4632\n",
      "Epoch   1 Batch   50/1077 - Loss: 0.4417\n",
      "Epoch   1 Batch  100/1077 - Loss: 0.4287\n",
      "Epoch   1 Batch  150/1077 - Loss: 0.3953\n",
      "Epoch   1 Batch  200/1077 - Loss: 0.3701\n",
      "Epoch   1 Batch  250/1077 - Loss: 0.3685\n",
      "Epoch   1 Batch  300/1077 - Loss: 0.3608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-24fe3082b9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0msource_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msource_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mtarget_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             })\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i ,(source_batch, target_batch, source_length, target_length) in enumerate(\n",
    "            get_batch(source_text_to_int, target_text_to_int, batch_size)):\n",
    "            \n",
    "            _, loss = sess.run([training_op, cost], feed_dict={\n",
    "                inputs:source_batch,\n",
    "                targets:target_batch, \n",
    "                learning_rate:lr,\n",
    "                source_sequence_length: source_length,\n",
    "                target_sequence_length: target_length,\n",
    "            })\n",
    "            \n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {inputs: source_batch,\n",
    "                     source_sequence_length: source_length,\n",
    "                     target_sequence_length: target_length})\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_text_to_int) // batch_size, loss))\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"checkpoints/dev\")\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, source_vocab_to_int):\n",
    "    \"\"\"\n",
    "    将句子转化为数字编码\n",
    "    \"\"\"\n",
    "    unk_idx = source_vocab_to_int[\"<UNK>\"]\n",
    "    word_idx = [source_vocab_to_int.get(word, unk_idx) for word in sentence.lower().split()]\n",
    "    \n",
    "    return word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_sentence_text = input(\"请输入句子：\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_sentence = sentence_to_seq(translate_sentence_text, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph('checkpoints/dev.meta')\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('./checkpoints'))\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_len:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_len:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size})[0]\n",
    "\n",
    "print('【Input】')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\n【Prediction】')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format([target_int_to_vocab[i] for i in translate_logits]))\n",
    "\n",
    "print(\"\\n【Full Sentence】\")\n",
    "print(\" \".join([target_int_to_vocab[i] for i in translate_logits]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
